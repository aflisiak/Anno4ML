# -*- coding: utf-8 -*-
"""PUBLIC_Assignment-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uQ197hCB_pOvvIuTy1NIq44LWQ0x8nDR

**Name_Surname (s-number), Name_Surname (s-number), Name_Surname (s-number)**

!!! Write here who contributed to which exercise and how (don't be too specific)

# Prelude
"""

# Necessary modules (optionals are commented out)
import os
import sys
from os import path as op
from collections import Counter, defaultdict
import re
import spacy
spacy.cli.download("en_core_web_sm")
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
nltk.download('averaged_perceptron_tagger')
import warnings # to silence stanford pos tagger

# Environment info (DELETEME)
print(f"Python version: {sys.version}")
print(f"NLTK version: {nltk.__version__}")
print(f"SpaCy: {spacy.__version__}")

# IMPORT Google drive
from google.colab import drive
drive.mount('/content/gdrive')

!unzip pmb.zip

# Commented out IPython magic to ensure Python compatibility.
# %  ls PMB_DIR

# Commented out IPython magic to ensure Python compatibility.
# % less ../pmb/p27/d0857/nl.raw

d = defaultdict(list)
d['newkey1'].append('newtoken')
d['newkey2']

d

"""!!! **SET ENVIRONMENT VARIABLES**"""

# Change me based on where this notebook is
THIS_DIR = '/content/gdrive/My Drive/Anno4ML/assign-1/PUBLIC' 
PMB_DIR = '../pmb'  # you can change this
TOOL_DIR = '../'    # you can change this

os.chdir(THIS_DIR)
print(f"Current working dir: {os.getcwd()}")

"""---
# Ex1: Reading and analyzing a corpus
---

## 1A: Read pmb directory in a dictionary
"""

# Reading the directory will take some time 
# So you might want to initially limit your code to particular pXX
# before the code will be functioning properly
# write pmb.txt file in the current directory
pmb['01/0777']



"""## 1B: Write pmb dictionary in a single file"""

# write pmb dictionary in a single file

# Commented out IPython magic to ensure Python compatibility.
# % less pmb.txt

"""## 1C: top 10 words"""

# You can also print out table in an ASCII if you want
#    DE        EN          IT         NL
# token 99 | token 99 | token 99 | token 99 | 
# token 00 | token 99 | token 99 | token 99 |
# or use the mardown as shown below

"""DE | EN | IT | NL 
--- | --- | --- | ---
token 99 | token 99 | token 99 | token 99
token 99 | token 99 | token 99 | token 99
token 99 | token 99 | token 99 | token 99

Pointing out major similarities and differences (max 100 words).

---
# Ex2: Stand-off tokenization
---

### Read pmb.txt file
"""

# read pmb.txt data in pmb dictionary
def read_pmb_from_file(pmb_file):
  with open(pmb_file) as FILE:
    pmb_list = re.split('\n### (.+) ###\n', '\n'+FILE.read())[1:]
  pmb = defaultdict(dict)
  for info, raw in zip(pmb_list[::2], pmb_list[1::2]):
    pd, lang, corpus = info.split()
    pmb[pd]['met'] = corpus
    pmb[pd][lang] = raw
  return pmb

pmb = read_pmb_from_file('pmb.txt')
print(f"Number of docs = {len(pmb)}")

pmb['03/0937']

"""## 2A: Tokenize pmb dictionary with NLTK or SpaCy"""

sample_sentence = "John doesn't live in U.S. anymore.\nHe  has moved to Groningen"

# Example of running sentence and word tokenizers from NLTK
for sen in sent_tokenize(sample_sentence):
    print(word_tokenize(sen))

# Use SpaCy sentence and word tokenizer
# This is not the most effcient way of running it 
# but it is simple and feasible for our corpus 
nlp = spacy.load("en_core_web_sm")
for sen in nlp(sample_sentence).sents:
    print([ tok.text for tok in nlp(sen.text) ])

nltk_sen_tok = pmb_en_seg(pmb, 'nltk')
spcy_sen_tok = pmb_en_seg(pmb, spacy.load("en_core_web_sm"))

nltk_sen_tok['01/0777']

spcy_sen_tok['01/0777']

"""## 2B: Convert token list into TOIS stand-off annotation"""

nltk_seg = sen_tok_to_off_seg(pmb, nltk_sen_tok, 'en.nltk.seg')
spcy_seg = sen_tok_to_off_seg(pmb, spcy_sen_tok, 'en.spcy.seg')

pmb['01/0777']['EN']

nltk_seg['01/0777']

# Commented out IPython magic to ensure Python compatibility.
# % less en.spcy.raw.seg

"""## 2C*: Remedy for altering double quotes"""

def whatever_functions_you_want_to_define(x):
    # Insert this in sen_tok_to_off_seg to fix the mentioned shortcoming
    return None

"""---
# Ex3: Part-of-speech tagging
---

## 3A: Read token lists from stand-off segementation
"""

nltk_sen_tok = apply_off_seg('pmb.txt', 'en.nltk.seg')
spcy_sen_tok = apply_off_seg('pmb.txt', 'en.spcy.seg')

"""## 3B: Tag data with 5 taggers

You need to obtain the necessary files for the POS taggers (links are in the function comments). **Don't change these functions**. Put the downloaded files in appropriate locations that the provided functions will work without changing.

Also you need to make the following files executable. Put correct paths there and execute the provided command.
"""

! chmod +x ../hunpos/hunpos-1.0-linux/hunpos-*; chmod +x ../senna/senna-linux64

# Commented out IPython magic to ensure Python compatibility.
# % ls ../hunpos/hunpos-1.0-linux/hun

! tar -xzf ../hunpos/

def hun_pos_tagger(sents, progress=False):
    #https://code.google.com/archive/p/hunpos/downloads
    hp = nltk.tag.HunposTagger(op.join(TOOL_DIR, 'hunpos/english.model'), 
                               path_to_bin=op.join(TOOL_DIR, 'hunpos/hunpos-1.0-linux/hunpos-tag'), 
                               encoding='utf-8')
    pos_tags = []
    for i, sen in enumerate(sents, start=1):
        if progress and i % 100 == 0: print('.', end='')
        pos_sen = [ pos.decode('utf-8') for (_, pos) in hp.tag(sen) ]
        pos_tags.append(pos_sen)
    return pos_tags

def nltk_pos_tagger(sents, progress=False):
    '''Uses averaged_perceptron_tagger'''
    #nltk.download('averaged_perceptron_tagger')
    pos_tags = []
    for i, sen in enumerate(sents, start=1):
        if progress and i % 100 == 0: print('.', end='')
        pos_sen = [ pos for (_, pos) in nltk.pos_tag(sen) ]
        pos_tags.append(pos_sen)
    return pos_tags

def senna_pos_tagger(sents, progress=False):
    #https://ronan.collobert.com/senna/download.html
    senna_tagger = nltk.tag.SennaTagger(op.join(TOOL_DIR, 'senna'))
    pos_tags = []
    for i, sen in enumerate(sents, start=1):
        if progress and i % 100 == 0: print('.', end='')
        pos_sen = [ pos for (_, pos) in senna_tagger.tag(sen) ]
        pos_tags.append(pos_sen) 
    return pos_tags 

def stanford_pos_tagger(sents, progress=False):
    #https://nlp.stanford.edu/software/tagger.html#Download
    model = op.join(TOOL_DIR, 'stanford-postagger-full-2018-10-16/models/wsj-0-18-bidirectional-distsim.tagger')
    jar_path = op.join(TOOL_DIR, 'stanford-postagger-full-2018-10-16/stanford-postagger.jar')
    # ignore depriciate warning
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("ignore")
        stanford_tagger = nltk.tag.StanfordPOSTagger(model, path_to_jar=jar_path)
    pos_tags = []
    for i, sen in enumerate(sents, start=1):
        if progress and i % 100 == 0: print('.', end='')
        pos_sen = [ pos for (_, pos) in stanford_tagger.tag(sen) ]
        pos_tags.append(pos_sen) 
    return pos_tags

def spacy_pos_tagger(sents, progress=False):
    #https://spacy.io/usage/spacy-101#annotations-pos-deps
    nlp = spacy.load('en_core_web_sm')
    pos_tags = []
    for i, sen in enumerate(sents, start=1):
        if progress and i % 100 == 0: print('.', end='')
        input = spacy.tokens.doc.Doc(nlp.vocab, sen)
        pos_sen = [ t.tag_ for t in nlp.tagger(input) ]
        pos_tags.append(pos_sen) 
    return pos_tags

sample_docs = ["This is a sample text .".split(), "There are no non ascii symbols".split()]
taggers = (hun_pos_tagger, nltk_pos_tagger, senna_pos_tagger, stanford_pos_tagger, spacy_pos_tagger)
for pos_tagger in taggers:
    print(f"{pos_tagger.__name__}:\t{pos_tagger(sample_docs)}")

"""---
# Ex4: Compare Annotations
---

## 4A: Contrast Segmentations
"""

contrast_seg('pmb.txt', 'en.nltk.seg', 'en.spcy.seg', 'seg.html')

"""## 4B: Discussing different types of segemntation differences"""

# code that reports the total number of TOIS labels 
# and the counts and % when the two segementations differ

"""Discussion of differences goes here

## 4C: Contrast 5 taggers
"""

# pos5_nltk shows that nltk segmentation was used
# to obtain tokens for POS tagging
contrast_pos(tagged5_nltk, 'pmb.txt', 'pos5_nltk.html')

"""## 4D Different POS tag voting scenarios"""

# You can print a table structure using the code
# or make the raw numbers print and then put them nicely in the table below

"""Scenario | Count | % 
--- | ---   | --- 
Total       | 999 | 10%
Unanimous   | 999 | 10%
4-1      | 999 | 10%
3-2      | 999 | 10%
3-1-1 | 999 | 10%

Order the scenarios according to their names, for example, 4-1 > 3-2 > 3-1-1 > 2-2-1 > 1-1-1-1-1. In the name numbers are always sorted X-Y, where X >= Y

## 4E: Discussing different types of POS tagging votes

Discussion of differences and the arguments for correct POS tags go here

For the sample file `pos_5_nltk.html`, the following explanations are satisfactory.

1. ***sample*** should be NN as "*Nouns that are used as modifiers, whether  in isolation or in sequences, should be  tagged as nouns (NN, NNS) rather than as adjectives (JJ)*" (see p12).  
2. ***non*** should be FW as "*Use your judgment as to what is a foreign  word.  For me, yoga is an NN,  while bete noire and persona non grata should be tagged bete/FW noire/FW and persona/FW non/FW grata/FW, respectively.*" (see p3). 
3. ***ascii*** should be NP as "*Abbreviations and initials should be tagged as if  they were spelled out*" (see p18 and p32).

The reference pages are from the [Part-of-Speech Tagging Guidelines for the Penn Treebank Project](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports).
"""

